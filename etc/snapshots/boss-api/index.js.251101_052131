import express from 'express';
import dotenv from 'dotenv';
import { execFile, spawn } from 'node:child_process';
import { promisify } from 'node:util';
import path from 'node:path';
import fs from 'node:fs/promises';
import { createWriteStream, constants as fsConstants } from 'node:fs';
import { fileURLToPath } from 'node:url';
import { randomUUID } from 'node:crypto';
import v2Router from './routes/v2.js';

dotenv.config();

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

const app = express();
const PORT = process.env.PORT || 4000;
const defaultSotPath = path.resolve(__dirname, '..', '..');
const SOT_PATH = process.env.SOT_PATH || defaultSotPath;
const pathResolverScript = path.resolve(__dirname, '..', '..', 'g', 'tools', 'path_resolver.sh');
const crawlScriptPath = path.resolve(__dirname, '..', '..', 'run', 'crawl.sh');
const ingestScriptPath = path.resolve(__dirname, '..', '..', 'crawler', 'ingest.py');
const trainScriptPath = path.resolve(__dirname, '..', '..', 'run', 'auto_train.sh');
const execFileAsync = promisify(execFile);

const PROCESS_START_TIMEOUT_MS = 60_000;
const MAX_LOG_LINES = 200;
const MAX_PAULA_JSON_BYTES = 1 * 1024 * 1024;

const CHILD_ENV_WHITELIST = new Set([
  'HOME',
  'LANG',
  'LC_ALL',
  'NODE_ENV',
  'PATH',
  'PYTHONPATH',
  'SHELL',
  'SOT_PATH',
  'TMPDIR',
  'PAULA_RUN_ID',
  'PAULA_JOB_TYPE'
]);

const allowedSeedOrigins = new Set(
  (process.env.PAULA_ALLOWED_SEEDS || 'https://theedges.work')
    .split(',')
    .map((value) => value.trim())
    .filter(Boolean)
);

const secretValuePatterns = Object.entries(process.env)
  .filter(([key, value]) => value && /KEY|SECRET|TOKEN|PASSWORD|API|ACCESS/i.test(key))
  .map(([, value]) => value)
  .filter((value) => typeof value === 'string' && value.length >= 4)
  .map((value) => new RegExp(value.replace(/[.*+?^${}()|[\]\\]/g, '\\$&'), 'g'));

const jobTable = new Map();
const jobLogDir = path.resolve(__dirname, '..', 'data', 'paula_logs');
fs.mkdir(jobLogDir, { recursive: true }).catch((error) => {
  console.error('Failed to ensure Paula log directory', error);
});

// Add middleware for better performance
app.get('/health',(req,res)=>res.status(200).send('ok'));

app.get('/metrics',(req,res)=>res.json({name:'boss-api',pid:process.pid,uptime:process.uptime(),rss:process.memoryUsage().rss}));

app.use(express.json({ limit: '10mb' }));
app.use(express.urlencoded({ extended: true, limit: '10mb' }));

// Add CORS headers for better performance
app.use((req, res, next) => {
  res.header('Access-Control-Allow-Origin', '*');
  res.header('Access-Control-Allow-Methods', 'GET, POST, PUT, DELETE, OPTIONS');
  res.header('Access-Control-Allow-Headers', 'Origin, X-Requested-With, Content-Type, Accept, Authorization');
  res.header('Cache-Control', 'public, max-age=300'); // 5 minutes cache
  if (req.method === 'OPTIONS') {
    res.sendStatus(200);
  } else {
    next();
  }
});

app.use('/api/v2', v2Router);

// Simple in-memory cache with size limits
const cache = new Map();
const CACHE_DURATION = 5 * 60 * 1000; // 5 minutes
const MAX_CACHE_SIZE = 100; // Maximum number of cached items
const MAX_CACHE_MEMORY = 50 * 1024 * 1024; // 50MB max cache memory

const allowedFolders = new Set([
  'inbox',
  'sent',
  'deliverables',
  'dropbox',
  'drafts',
  'documents'
]);

function buildChildEnv(additional = {}) {
  const env = {};
  for (const key of CHILD_ENV_WHITELIST) {
    if (process.env[key]) {
      env[key] = process.env[key];
    }
  }

  for (const [key, value] of Object.entries(additional)) {
    if (value === undefined || value === null) {
      continue;
    }
    if (!CHILD_ENV_WHITELIST.has(key)) {
      continue;
    }
    env[key] = value;
  }

  return env;
}

function redactSecrets(input) {
  if (!input || secretValuePatterns.length === 0) {
    return input;
  }

  let output = input;
  for (const pattern of secretValuePatterns) {
    output = output.replace(pattern, '***');
  }
  return output;
}

async function readLogTail(logPath) {
  try {
    const content = await fs.readFile(logPath, 'utf8');
    const lines = content.split(/\r?\n/);
    const tailLines = lines.slice(-MAX_LOG_LINES);
    return redactSecrets(tailLines.join('\n'));
  } catch (error) {
    if (error.code === 'ENOENT') {
      return '';
    }
    throw error;
  }
}

function queuePaulaJob({ jobType, command, args = [], cwd, env: envOverrides = {} }) {
  const runId = randomUUID();
  const logPath = path.join(jobLogDir, `${runId}.log`);
  const acceptedAt = new Date().toISOString();

  const record = {
    id: runId,
    job_type: jobType,
    status: 'queued',
    accepted_at: acceptedAt,
    started_at: null,
    ended_at: null,
    tail_log_path: logPath
  };

  jobTable.set(runId, record);

  setImmediate(() => {
    const childEnv = buildChildEnv({ ...envOverrides, PAULA_RUN_ID: runId, PAULA_JOB_TYPE: jobType });
    const logStream = createWriteStream(logPath, { flags: 'a' });
    logStream.on('error', (error) => {
      console.error('Failed writing Paula log', error);
    });

    const timestamp = new Date().toISOString();
    logStream.write(`[${timestamp}] Queued job ${jobType} (${runId})\n`);

    let finished = false;
    let spawnTimeout = null;

    const finalize = (status, extra = {}) => {
      if (finished) {
        return;
      }
      finished = true;
      if (spawnTimeout) {
        clearTimeout(spawnTimeout);
        spawnTimeout = null;
      }
      record.status = status;
      record.ended_at = new Date().toISOString();
      if (extra.exitCode !== undefined) {
        record.exit_code = extra.exitCode;
      }
      if (extra.signal !== undefined) {
        record.signal = extra.signal;
      }
      if (extra.error) {
        const sanitizedError = redactSecrets(extra.error);
        record.error = sanitizedError;
        logStream.write(`${sanitizedError}\n`);
      }
      logStream.write(`[${record.ended_at}] Completed with status ${status}\n`);
      logStream.end();
    };

    try {
      const child = spawn(command, args, {
        cwd,
        env: childEnv,
        stdio: ['ignore', 'pipe', 'pipe']
      });

      spawnTimeout = setTimeout(() => {
        logStream.write('Process failed to start within timeout.\n');
        child.kill('SIGKILL');
        finalize('fail', { error: 'Process start timeout' });
      }, PROCESS_START_TIMEOUT_MS);

      child.once('spawn', () => {
        if (spawnTimeout) {
          clearTimeout(spawnTimeout);
          spawnTimeout = null;
        }
        record.status = 'running';
        record.started_at = new Date().toISOString();
        logStream.write(`[${record.started_at}] Running command: ${command} ${args.join(' ')}\n`);
      });

      const writeChunk = (chunk) => {
        const text = typeof chunk === 'string' ? chunk : chunk.toString();
        if (!text) {
          return;
        }
        logStream.write(redactSecrets(text));
      };

      child.stdout.on('data', writeChunk);
      child.stderr.on('data', writeChunk);

      child.once('error', (error) => {
        finalize('fail', { error: error.message });
      });

      child.once('close', (code, signal) => {
        const status = code === 0 ? 'success' : 'fail';
        const error = code === 0 ? undefined : `Process exited with code ${code}${signal ? ` (signal ${signal})` : ''}`;
        finalize(status, { exitCode: code, signal, error });
      });
    } catch (error) {
      finalize('fail', { error: error.message });
    }
  });

  return record;
}

function buildError(message, code = 'internal_error') {
  return { message, code };
}

// Cache utility functions
function getCached(key) {
  const cached = cache.get(key);
  if (cached && (Date.now() - cached.timestamp) < CACHE_DURATION) {
    return cached.data;
  }
  cache.delete(key);
  return null;
}

function setCached(key, data) {
  // Check cache size limits
  if (cache.size >= MAX_CACHE_SIZE) {
    // Remove oldest entries
    const entries = Array.from(cache.entries());
    entries.sort((a, b) => a[1].timestamp - b[1].timestamp);
    const toRemove = Math.floor(MAX_CACHE_SIZE * 0.2); // Remove 20% of cache
    for (let i = 0; i < toRemove; i++) {
      cache.delete(entries[i][0]);
    }
  }
  
  // Check memory usage
  const dataSize = JSON.stringify(data).length;
  if (dataSize > MAX_CACHE_MEMORY / 10) { // If single item is > 10% of max memory
    console.warn(`Large cache item detected: ${dataSize} bytes`);
  }
  
  cache.set(key, {
    data,
    timestamp: Date.now(),
    size: dataSize
  });
}

// Clean up old cache entries periodically
setInterval(() => {
  const now = Date.now();
  let cleaned = 0;
  for (const [key, value] of cache.entries()) {
    if ((now - value.timestamp) > CACHE_DURATION) {
      cache.delete(key);
      cleaned++;
    }
  }
  if (cleaned > 0) {
    console.log(`Cache cleanup: removed ${cleaned} expired entries`);
  }
}, CACHE_DURATION);

// Memory monitoring and garbage collection
setInterval(() => {
  const memUsage = process.memoryUsage();
  const cacheSize = cache.size;
  const cacheMemory = Array.from(cache.values()).reduce((sum, item) => sum + (item.size || 0), 0);
  
  console.log(`Memory: RSS=${Math.round(memUsage.rss/1024/1024)}MB, Cache: ${cacheSize} items (${Math.round(cacheMemory/1024)}KB)`);
  
  // Force garbage collection if memory usage is high
  if (memUsage.rss > 100 * 1024 * 1024) { // 100MB
    if (global.gc) {
      global.gc();
      console.log('Forced garbage collection');
    }
  }
  
  // Clear cache if it's too large
  if (cacheMemory > MAX_CACHE_MEMORY) {
    console.log('Cache too large, clearing...');
    cache.clear();
  }
}, 30000); // Every 30 seconds

async function resolveFolder(folder) {
  if (!allowedFolders.has(folder)) {
    const error = new Error(`Unknown folder: ${folder}`);
    error.status = 400;
    error.code = 'unknown_folder';
    throw error;
  }

  const { stdout } = await execFileAsync('bash', [pathResolverScript, `human:${folder}`], {
    env: {
      ...process.env,
      SOT_PATH
    }
  });

  return stdout.trim();
}

function ensureChildPath(parent, child) {
  const resolved = path.resolve(parent, child);
  const relative = path.relative(parent, resolved);
  if (relative.startsWith('..') || path.isAbsolute(relative)) {
    const error = new Error('Invalid path traversal detected');
    error.status = 400;
    error.code = 'invalid_path';
    throw error;
  }
  return resolved;
}

const paulaRateLimitMap = new Map();
const PAULA_RATE_LIMIT_WINDOW = 60_000;
const PAULA_RATE_LIMIT_MAX = 60;

function applyPaulaRateLimit(req, res, next) {
  const key = req.ip || req.headers['x-forwarded-for'] || 'unknown';
  const now = Date.now();
  const windowStart = now - PAULA_RATE_LIMIT_WINDOW;

  if (!paulaRateLimitMap.has(key)) {
    paulaRateLimitMap.set(key, []);
  }

  const timestamps = paulaRateLimitMap.get(key).filter((timestamp) => timestamp > windowStart);
  if (timestamps.length >= PAULA_RATE_LIMIT_MAX) {
    return res.status(429).json(buildError('Rate limit exceeded', 'rate_limited'));
  }

  timestamps.push(now);
  paulaRateLimitMap.set(key, timestamps);
  return next();
}

const paulaRouter = express.Router();

paulaRouter.use(applyPaulaRateLimit);

paulaRouter.use((req, res, next) => {
  const contentLength = req.headers['content-length'];
  if (contentLength && Number(contentLength) > MAX_PAULA_JSON_BYTES) {
    return res.status(413).json(buildError('Payload too large', 'payload_too_large'));
  }
  return next();
});

paulaRouter.use((req, res, next) => {
  try {
    const payload = req.body ?? {};
    const serialized = JSON.stringify(payload);
    if (Buffer.byteLength(serialized || '', 'utf8') > MAX_PAULA_JSON_BYTES) {
      return res.status(413).json(buildError('Payload too large', 'payload_too_large'));
    }
  } catch (error) {
    return res.status(400).json(buildError('Invalid JSON body', 'invalid_json'));
  }
  return next();
});

paulaRouter.post('/crawl', async (req, res) => {
  try {
    const { seeds, max_pages, maxPages } = req.body || {};
    if (!Array.isArray(seeds) || seeds.length === 0) {
      return res.status(400).json(buildError('seeds array is required', 'invalid_payload'));
    }

    const normalizedSeeds = [];
    for (const seed of seeds) {
      if (typeof seed !== 'string') {
        return res.status(400).json(buildError('Seed values must be strings', 'invalid_seed'));
      }
      let seedUrl;
      try {
        seedUrl = new URL(seed);
      } catch (error) {
        return res.status(400).json(buildError(`Invalid seed URL: ${seed}`, 'invalid_seed_url'));
      }
      const origin = `${seedUrl.protocol}//${seedUrl.host}`;
      if (!allowedSeedOrigins.has(origin)) {
        return res.status(400).json(buildError(`Seed origin not allowed: ${origin}`, 'seed_not_allowed'));
      }
      normalizedSeeds.push(seed);
    }

    const maxPagesValue = max_pages ?? maxPages;
    if (maxPagesValue !== undefined) {
      const numericMax = Number(maxPagesValue);
      if (!Number.isFinite(numericMax) || numericMax <= 0) {
        return res.status(400).json(buildError('max_pages must be a positive number', 'invalid_max_pages'));
      }
    }

    await fs.access(crawlScriptPath, fsConstants.X_OK).catch(() => {
      throw new Error('Crawler script unavailable');
    });

    const args = [
      crawlScriptPath,
      normalizedSeeds.join(','),
    ];

    if (maxPagesValue !== undefined) {
      args.push(String(Math.floor(Number(maxPagesValue))));
    }

    const job = queuePaulaJob({
      jobType: 'crawl',
      command: 'bash',
      args,
      cwd: path.dirname(crawlScriptPath)
    });

    return res.status(202).json({ accepted: true, run_id: job.id });
  } catch (error) {
    console.error('Failed to queue crawl job', error);
    return res.status(500).json(buildError('Failed to start crawl job', 'crawl_failed'));
  }
});

paulaRouter.post('/ingest', async (req, res) => {
  try {
    const payload = req.body || {};

    await fs.access(ingestScriptPath, fsConstants.R_OK).catch(() => {
      throw new Error('Ingest script unavailable');
    });

    const args = [
      ingestScriptPath,
      '--payload-base64',
      Buffer.from(JSON.stringify(payload || {}), 'utf8').toString('base64')
    ];

    const job = queuePaulaJob({
      jobType: 'ingest',
      command: 'python3',
      args,
      cwd: path.dirname(ingestScriptPath)
    });

    return res.status(202).json({ accepted: true, run_id: job.id });
  } catch (error) {
    console.error('Failed to queue ingest job', error);
    return res.status(500).json(buildError('Failed to start ingest job', 'ingest_failed'));
  }
});

paulaRouter.post('/auto-train', async (req, res) => {
  try {
    const { strategy } = req.body || {};
    if (!strategy || typeof strategy !== 'string' || !strategy.trim()) {
      return res.status(400).json(buildError('strategy is required', 'invalid_strategy'));
    }

    await fs.access(trainScriptPath, fsConstants.X_OK).catch(() => {
      throw new Error('Training script unavailable');
    });

    const args = [trainScriptPath, '--strategy', strategy.trim()];

    const job = queuePaulaJob({
      jobType: 'auto-train',
      command: 'bash',
      args,
      cwd: path.dirname(trainScriptPath)
    });

    return res.status(202).json({ accepted: true, run_id: job.id });
  } catch (error) {
    console.error('Failed to queue training job', error);
    return res.status(500).json(buildError('Failed to start training job', 'train_failed'));
  }
});

paulaRouter.get('/jobs/:id', async (req, res) => {
  try {
    const jobId = req.params.id;
    const job = jobTable.get(jobId);
    if (!job) {
      return res.status(404).json(buildError('Job not found', 'job_not_found'));
    }

    let tailLog = '';
    try {
      tailLog = await readLogTail(job.tail_log_path);
    } catch (error) {
      console.error('Failed to read Paula job log', error);
    }

    return res.json({
      id: job.id,
      job_type: job.job_type,
      status: job.status,
      accepted_at: job.accepted_at,
      started_at: job.started_at,
      ended_at: job.ended_at,
      exit_code: job.exit_code,
      signal: job.signal,
      error: job.error,
      tail_log_path: job.tail_log_path,
      tail_log: tailLog
    });
  } catch (error) {
    console.error('Failed to read Paula job status', error);
    return res.status(500).json(buildError('Failed to read job status', 'job_status_failed'));
  }
});

app.use('/api/paula', paulaRouter);

app.get('/api/list/:folder', async (req, res) => {
  try {
    const folderKey = req.params.folder;
    const cacheKey = `list-${folderKey}`;
    
    // Check cache first
    const cached = getCached(cacheKey);
    if (cached) {
      return res.json(cached);
    }

    const folderPath = await resolveFolder(folderKey);
    await fs.access(folderPath);
    const entries = await fs.readdir(folderPath, { withFileTypes: true });
    const files = entries
      .filter((entry) => entry.isFile())
      .map((entry) => ({ name: entry.name }))
      .sort((a, b) => a.name.localeCompare(b.name)); // Sort for consistent ordering

    const result = { files };
    setCached(cacheKey, result);
    res.json(result);
  } catch (error) {
    if (error.code === 'ENOENT') {
      error.status = error.status || 404;
      error.code = 'folder_not_found';
    }
    const status = error.status || 500;
    res.status(status).json(buildError(error.message, error.code || 'internal_error'));
  }
});

app.get('/api/file/:folder/:name', async (req, res) => {
  try {
    const { folder, name } = req.params;
    const cacheKey = `file-${folder}-${name}`;
    
    // Check cache first
    const cached = getCached(cacheKey);
    if (cached) {
      return res.json(cached);
    }

    const folderPath = await resolveFolder(folder);
    const filePath = ensureChildPath(folderPath, name);
    
    // Get file stats for better caching
    const stats = await fs.stat(filePath);
    const content = await fs.readFile(filePath, 'utf8');

    const result = { name, content, size: stats.size, modified: stats.mtime };
    setCached(cacheKey, result);
    res.json(result);
  } catch (error) {
    const status = error.status || (error.code === 'ENOENT' ? 404 : 500);
    const code = error.code === 'ENOENT' ? 'file_not_found' : error.code || 'internal_error';
    res.status(status).json(buildError(error.message, code));
  }
});

app.use((error, req, res, next) => {
  console.error('API error', error);
  if (res.headersSent) {
    return next(error);
  }
  const status = error.status || 500;
  const code = error.code || 'internal_error';
  const message = error.message || 'Unexpected error';
  res.status(status).json(buildError(message, code));
});

app.get('/version',(req,res)=>res.json({name:'boss-api',commit:(process.env.GIT_COMMIT||'local'),time:new Date().toISOString()}));

app.use((req, res) => {
  res.status(404).json(buildError('Not found', 'not_found'));
});




app.listen(PORT, () => {
  // eslint-disable-next-line no-console
  console.log(`Boss API listening on port ${PORT}`);
});
