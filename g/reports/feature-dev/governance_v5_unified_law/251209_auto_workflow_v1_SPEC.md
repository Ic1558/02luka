# Auto Workflow v1 â€” Fully Automatic Execution

**Date:** 2025-12-10  
**Status:** ğŸ“‹ SPEC  
**Authority:** Boss Directive

---

## ğŸ¯ Core Principle

**Fully Automatic from Beginning to End:**
- No Boss approval required at any step
- Auto-redesign if quality not met
- Final report generated automatically
- Boss only reviews final result

---

## ğŸ”„ Workflow Stages

### Stage 1: Design Phase (Auto)
```
PLAN â†’ SPEC â†’ REVIEW â†’ [Quality Gate] â†’ Continue or REDESIGN
```

### Stage 2: Implementation Phase (Auto)
```
DRYRUN â†’ CODE-REVIEW (Gate 2.5) â†’ VERIFY â†’ [Quality Gate] â†’ Continue or REDESIGN
```

### Stage 3: Execution Phase (Auto)
```
IMPLEMENT â†’ TEST â†’ [Gate 3] â†’ Continue or REDESIGN
```

### Stage 4: Finalization (Auto)
```
VALIDATE â†’ SCORE â†’ [Gate 4: Production Readiness â‰¥90%] â†’ DONE or REDESIGN
```

**Tool:** `zsh tools/run_tool.zsh feature-dev-validate <feature-slug>`
**Auto Executor:** `zsh tools/run_tool.zsh auto-workflow <feature-slug>` (includes validation automatically)

---

## ğŸ¯ Quality Gates

### Gate 1: Design Quality
**Check:** PLAN + SPEC completeness
- âœ… All required sections present
- âœ… Integration points defined
- âœ… Test strategy defined
- âœ… Success criteria measurable

**If FAIL:** â†’ REDESIGN (improve PLAN/SPEC)

---

### Gate 2.5: Code Review (Fast Gate)
**Check:** Code review gate (style, history, bugs)
- âœ… Style check passed
- âœ… No obvious bugs
- âœ… No critical security issues
- âœ… Diff hotspots identified

**Tool:** `zsh tools/code_review_gate.zsh <target>`
**Cache:** Uses `g/.cache/code_review_cache.json` for fast lookup
**Catalog:** Uses `catalog_lookup.zsh` for tool discovery (single source of truth)

**If FAIL:** â†’ REDESIGN (fix issues) or continue with warnings

---

### Gate 2: Code Quality
**Check:** DRYRUN code quality
- âœ… All functions implemented
- âœ… Integration points correct
- âœ… Error handling present
- âœ… Documentation complete
- âœ… Code review passed (Gate 2.5)

**If FAIL:** â†’ REDESIGN (improve code)

---

### Gate 3: Verification Quality
**Check:** VERIFY results
- âœ… Score >= 90/100
- âœ… No critical blockers
- âœ… All test cases pass

**If FAIL:** â†’ REDESIGN (fix issues)

---

### Gate 4: Implementation Quality
**Check:** IMPLEMENT results
- âœ… Files created successfully
- âœ… No linter errors
- âœ… Integration tests pass

**If FAIL:** â†’ REDESIGN (fix implementation)

---

### Gate 5: Final Report Quality
**Check:** REPORT completeness
- âœ… All stages documented
- âœ… Results summarized
- âœ… Next steps clear

**If FAIL:** â†’ REDESIGN (improve report)

---

## ğŸ”„ Auto-Redesign Logic

### Redesign Triggers
1. Quality gate fails
2. Score < 90/100
3. Critical blockers found
4. Integration failures

### Redesign Process
```
1. Analyze failure reason
2. Identify root cause
3. Redesign affected component
4. Retry from appropriate stage
5. Max retries: 3
```

### Redesign Strategy
- **Design failures:** Improve PLAN/SPEC
- **Code failures:** Fix implementation
- **Integration failures:** Fix integration points
- **Quality failures:** Enhance quality

---

## ğŸ“Š Final Report Format

### Automatic Report Generation
```markdown
# [Feature Name] â€” Auto Execution Report

**Date:** YYYY-MM-DD
**Status:** âœ… COMPLETE / âš ï¸ PARTIAL / âŒ FAILED
**Score:** X/100

## Execution Summary
- Stages completed: X/Y
- Redesigns: N
- Final status: [Status]

## Results
- [Stage 1]: âœ… PASS
- [Stage 2]: âœ… PASS
- [Stage 3]: âœ… PASS

## Files Created
- [List of files]

## Next Steps
- [Auto-generated recommendations]
```

---

## ğŸ¯ Implementation Rules

### Rule 1: No Boss Approval Required
- âœ… All stages execute automatically
- âœ… No "ASK BOSS" steps
- âœ… Final report only

### Rule 2: Auto-Redesign on Failure
- âœ… Quality gate fails â†’ Auto-redesign
- âœ… Max 3 retries per stage
- âœ… If still fails â†’ Report failure with analysis

### Rule 3: Quality Threshold
- âœ… Minimum score: 90/100
- âœ… No critical blockers
- âœ… All integrations working

### Rule 4: Final Report Always Generated
- âœ… Report created regardless of success/failure
- âœ… Includes full execution history
- âœ… Includes redesign attempts
- âœ… Includes recommendations

---

## ğŸ”§ Execution Flow

```
START
  â†“
PLAN (auto)
  â†“
SPEC (auto)
  â†“
REVIEW (auto)
  â†“
[Quality Gate 1]
  â”œâ”€ PASS â†’ Continue
  â””â”€ FAIL â†’ REDESIGN â†’ Retry from PLAN
  â†“
DRYRUN (auto)
  â†“
VERIFY (auto)
  â†“
[Quality Gate 2]
  â”œâ”€ PASS â†’ Continue
  â””â”€ FAIL â†’ REDESIGN â†’ Retry from DRYRUN
  â†“
IMPLEMENT (auto)
  â†“
TEST (auto)
  â†“
[Quality Gate 3]
  â”œâ”€ PASS â†’ Continue
  â””â”€ FAIL â†’ REDESIGN â†’ Retry from IMPLEMENT
  â†“
REPORT (auto)
  â†“
[Quality Gate 4]
  â”œâ”€ PASS â†’ DONE
  â””â”€ FAIL â†’ REDESIGN â†’ Retry from REPORT
  â†“
FINAL REPORT
```

---

## ğŸ“ Quality Gate Implementation

### Quality Gate Function
```python
def check_quality_gate(stage: str, result: Dict) -> Tuple[bool, List[str]]:
    """
    Check quality gate for stage.
    
    Returns:
        (passed, issues)
    """
    issues = []
    
    if stage == "DESIGN":
        # Check PLAN + SPEC completeness
        if not result.get("plan_complete"):
            issues.append("PLAN incomplete")
        if not result.get("spec_complete"):
            issues.append("SPEC incomplete")
        if result.get("score", 0) < 80:
            issues.append(f"Design score too low: {result.get('score')}")
    
    elif stage == "CODE":
        # Check DRYRUN code quality
        if not result.get("all_functions_implemented"):
            issues.append("Missing functions")
        if result.get("linter_errors"):
            issues.append(f"Linter errors: {len(result['linter_errors'])}")
        if result.get("score", 0) < 90:
            issues.append(f"Code score too low: {result.get('score')}")
    
    elif stage == "VERIFY":
        # Check verification results
        if result.get("score", 0) < 90:
            issues.append(f"Verification score too low: {result.get('score')}")
        if result.get("critical_blockers"):
            issues.append(f"Critical blockers: {len(result['critical_blockers'])}")
    
    elif stage == "IMPLEMENT":
        # Check implementation results
        if not result.get("files_created"):
            issues.append("No files created")
        if result.get("errors"):
            issues.append(f"Implementation errors: {len(result['errors'])}")
    
    passed = len(issues) == 0
    return (passed, issues)
```

---

## ğŸ”„ Auto-Redesign Implementation

### Redesign Function
```python
def auto_redesign(stage: str, failure_reason: List[str]) -> Dict:
    """
    Auto-redesign based on failure reason.
    
    Returns:
        Redesign plan
    """
    redesign_plan = {
        "stage": stage,
        "failure_reasons": failure_reason,
        "redesign_strategy": [],
        "retry_count": 0
    }
    
    # Analyze failure and create redesign strategy
    for reason in failure_reason:
        if "incomplete" in reason.lower():
            redesign_plan["redesign_strategy"].append("Enhance completeness")
        elif "score" in reason.lower():
            redesign_plan["redesign_strategy"].append("Improve quality")
        elif "error" in reason.lower():
            redesign_plan["redesign_strategy"].append("Fix errors")
        elif "blocker" in reason.lower():
            redesign_plan["redesign_strategy"].append("Resolve blockers")
    
    return redesign_plan
```

---

## ğŸ“Š Final Report Template

```markdown
# [Feature Name] â€” Auto Execution Report

**Date:** YYYY-MM-DD HH:MM:SS
**Status:** âœ… COMPLETE / âš ï¸ PARTIAL / âŒ FAILED
**Final Score:** X/100
**Total Time:** X hours Y minutes
**Redesigns:** N

---

## Execution Summary

### Stages Completed
- âœ… PLAN: Complete (Score: X/100)
- âœ… SPEC: Complete (Score: X/100)
- âœ… REVIEW: Complete (Score: X/100)
- âœ… DRYRUN: Complete (Score: X/100)
- âœ… VERIFY: Complete (Score: X/100)
- âœ… IMPLEMENT: Complete
- âœ… REPORT: Complete

### Redesign History
- Redesign #1: [Stage] - [Reason] - [Result]
- Redesign #2: [Stage] - [Reason] - [Result]

---

## Results

### Files Created
- `path/to/file1.py` (X lines)
- `path/to/file2.zsh` (X lines)
- `path/to/file3.md` (X lines)

### Quality Scores
- Design: X/100
- Code: X/100
- Verification: X/100
- Overall: X/100

### Test Results
- Unit Tests: X/Y passed
- Integration Tests: X/Y passed
- Performance Tests: X/Y passed

---

## Issues & Resolutions

### Issues Found
1. [Issue description] â†’ [Resolution]

### Resolutions Applied
1. [Resolution description] â†’ [Result]

---

## Next Steps

### Immediate
- [Auto-generated next steps]

### Recommendations
- [Auto-generated recommendations]

---

## Execution Log

```
[Timestamp] PLAN: Started
[Timestamp] PLAN: Complete (Score: X/100)
[Timestamp] SPEC: Started
[Timestamp] SPEC: Complete (Score: X/100)
[Timestamp] REVIEW: Started
[Timestamp] REVIEW: Complete (Score: X/100)
[Timestamp] DRYRUN: Started
[Timestamp] DRYRUN: Complete (Score: X/100)
[Timestamp] VERIFY: Started
[Timestamp] VERIFY: Complete (Score: X/100)
[Timestamp] IMPLEMENT: Started
[Timestamp] IMPLEMENT: Complete
[Timestamp] REPORT: Generated
```

---

**Status:** âœ… COMPLETE

**Last Updated:** YYYY-MM-DD HH:MM:SS
```

---

## ğŸ¯ Implementation Status

**Workflow:** Fully Automatic  
**Boss Approval:** Not Required  
**Auto-Redesign:** Enabled  
**Final Report:** Always Generated

---

**Status:** âœ… SPEC Complete â€” Ready for Implementation

